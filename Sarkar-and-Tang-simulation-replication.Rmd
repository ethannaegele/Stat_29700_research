---
title: "Sarkar and Tang simulation replication"
author: "Ethan Naegele"
date: "2024-04-19"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(knockoff)
```


# Internal functions in knockoffs package for computation purposes

```{r}
# Fast versions of diag(d) %*% X and X %*% diag(d).
`%diag*%` <- function(d, X) d * X
`%*diag%` <- function(X, d) t(t(X) * d)
```

```{r}
is_posdef = function(A, tol=1e-9) {
  p = nrow(matrix(A))
  
  if (p<500) {
    lambda_min = min(eigen(A)$values)
  }
  else {
    oldw <- getOption("warn")
    options(warn = -1)
    lambda_min = RSpectra::eigs(A, 1, which="SM", opts=list(retvec = FALSE, maxitr=100, tol))$values
    options(warn = oldw)
    if( length(lambda_min)==0 ) {
      # RSpectra::eigs did not converge. Using eigen instead."
      lambda_min = min(eigen(A)$values)
    }
  }
  return (lambda_min>tol*10)
}
```

```{r}
# Scale the columns of a matrix to have unit norm.
normc = function(X,center=T) {
  X.centered = scale(X, center=center, scale=F)
  X.scaled = scale(X.centered, center=F, scale=sqrt(colSums(X.centered^2)))
  X.scaled[,] # No attributes
}
```



```{r}
# Reduced SVD with canonical sign choice.
# 
# Our convention is that the sign of each vector in U is chosen such that the
# coefficient with the largest absolute value is positive.
canonical_svd = function(X) {
  X.svd = tryCatch({
    svd(X)
  }, warning = function(w){}, error = function(e) {
      stop("SVD failed in the creation of fixed-design knockoffs. Try upgrading R to version >= 3.3.0")
  }, finally = {})
  
  for (j in 1:min(dim(X))) {
    i = which.max(abs(X.svd$u[,j]))
    if (X.svd$u[i,j] < 0) {
      X.svd$u[,j] = -X.svd$u[,j]
      X.svd$v[,j] = -X.svd$v[,j]
  }
    }
  return(X.svd)
}

```


```{r}
#' Compute the SVD of X and construct an orthogonal matrix U_perp such that U_perp * U = 0.
#'  
decompose <- function(X, randomize=FALSE) {
  n = nrow(X); p = ncol(X)
  stopifnot(n >= 2*p)
  
  result = canonical_svd(X)
  Q = qr.Q(qr(cbind(result$u, matrix(0,n,p))))
  u_perp = Q[,(p+1):(2*p)]
  if (randomize) {
      Q = qr.Q(qr(rnorm_matrix(p,p)))
      u_perp = u_perp %*% Q
  }
  result$u_perp = u_perp
  result
}
```


```{r}
# I created the first function from the knockoffs package code
create_equicorrelated_D <- function(X, randomize=FALSE) {
  # Compute SVD and U_perp.
  X = normc(X, center=F)
  X.svd = decompose(X, randomize)
  
  # Set s = min(2 * smallest eigenvalue of X'X, 1), so that all the correlations
  # have the same value 1-s.
  if (any(X.svd$d <= 1e-5 * max(X.svd$d)))
    stop(paste('Data matrix is rank deficient.',
               'Equicorrelated knockoffs will have no power.'))
  lambda_min = min(X.svd$d)^2
  s = min(2*lambda_min, 1)
  s_vec <- rep(s, times = ncol(X))
  return(diag(s_vec))
}


create_equicorrelated_knockoffs <- function(X, randomize=FALSE) {
  # Compute SVD and U_perp.
  X = normc(X, center=F)
  X.svd = decompose(X, randomize)
  
  # Set s = min(2 * smallest eigenvalue of X'X, 1), so that all the correlations
  # have the same value 1-s.
  if (any(X.svd$d <= 1e-5 * max(X.svd$d)))
    stop(paste('Data matrix is rank deficient.',
               'Equicorrelated knockoffs will have no power.'))
  lambda_min = min(X.svd$d)^2
  s = min(2*lambda_min, 1)
  
  # Construct the knockoff according to Equation 1.4.
  s_diff = pmax(0, 2*s - (s/X.svd$d)^2) # can be negative due to numerical error
  X_ko = (X.svd$u %*diag% (X.svd$d - s / X.svd$d) +
          X.svd$u_perp %*diag% sqrt(s_diff)) %*% t(X.svd$v)
  return(X_ko)
}

```




```{r}
# THIS IS A FUNCTION FROM THE PACKAGE, BUT I CHANGED THE OUTPUT TO BE DIAG(S)
create.solve_equi <- function(Sigma) {
  # Check that covariance matrix is symmetric
  stopifnot(isSymmetric(Sigma))
  p = nrow(Sigma)
  tol = 1e-10
  # Convert the covariance matrix to a correlation matrix
  G = cov2cor(Sigma)
  
  # Check that the input matrix is positive-definite
  if (!is_posdef(G)) {
    stop('The covariance matrix is not positive-definite: cannot solve SDP',immediate.=T)
  }
  
  if (p>2) {
    converged=FALSE
    maxitr=10000
    while (!converged) {
      lambda_min = RSpectra::eigs(G, 1, which="SR", opts=list(retvec = FALSE, maxitr=100000, tol=1e-8))$values
      if (length(lambda_min)==1) {
        converged = TRUE
      } else {
        if (maxitr>1e8) {
          warning('In creation of equi-correlated knockoffs, while computing the smallest eigenvalue of the 
                covariance matrix. RSpectra::eigs did not converge. Giving up and computing full SVD with built-in R function.',immediate.=T)
          lambda_min = eigen(G, symmetric=T, only.values = T)$values[p]
          converged=TRUE
        } else {
          warning('In creation of equi-correlated knockoffs, while computing the smallest eigenvalue of the 
                covariance matrix. RSpectra::eigs did not converge. Trying again with increased number of iterations.',immediate.=T)
          maxitr = maxitr*10
        }
      }
    }
  } else {
    lambda_min = eigen(G, symmetric=T, only.values = T)$values[p]
  }
  
  if (lambda_min<0) {
    stop('In creation of equi-correlated knockoffs, while computing the smallest eigenvalue of the 
                covariance matrix. The covariance matrix is not positive-definite.')
  }
  
  s = rep(1, nrow(Sigma)) * min(2*lambda_min, 1)
  
  # Compensate for numerical errors (feasibility)
  psd = 0;
  s_eps = 1e-8;
  while (psd==0) {
    psd = is_posdef(2*G-diag(s*(1-s_eps),length(s)))
    if (!psd) {
      s_eps = s_eps*10
    }
  }
  s = s*(1-s_eps)
  
  # Scale back the results for a covariance matrix
  return(diag(s))
}
```




# Functions specific to quantities from Sarkar and Tang

```{r}
# faster function for calculating norm of vector
norm_vec <- function(x) sqrt(sum(x^2))

# obtain the unbiased estimator of the variance parameter tau in the regression setting Y = N(X * beta, tau^2 I_n)
get_variance_parameter_estimate <- function(X, Y){
  n <- nrow(X)
  d <- ncol(X)
  tau_hat_squared <- (norm_vec(Y - X %*% solve(t(X) %*% X) %*% t(X) %*% Y))^2 / (n - d) # numerator is RSS, and assuming n > d
  return(sqrt(tau_hat_squared))
}
```



```{r}
# get the T_1  estimate provided at the beginning of section 2
library(expm)
get_T_1 <- function(X, Y){
  sigma <- solve(t(X) %*% X)
  D <- create_equicorrelated_D(X)
  X_ko <- create_equicorrelated_knockoffs(X)
  beta_hat_1 <- solve(2 * sigma - D) %*% t(X + X_ko) %*% Y
  # tau hat estimated with the columnwise concatenation of X and knockoff X
  T_1 <- (1 / get_variance_parameter_estimate(cbind(X, X_ko), Y) * sqrt(2)) *
    solve(sqrtm(solve(diag(diag(2 * sigma - D))))) %*% beta_hat_1 # unsure about interpretation of diag operator in the equation for T1 from the paper; this is my best interpretation of it - Ignat agrees
}

# get the T_2 estimate 
get_T_2 <- function(X, Y){
  D <- create_equicorrelated_D(X)
  X_ko <- create_equicorrelated_knockoffs(X)
  beta_hat_2 <- solve(D) %*% t(X - X_ko) %*% Y
  T_2 <- (1 / get_variance_parameter_estimate(cbind(X, X_ko), Y) * sqrt(2)) * sqrtm(D) %*% beta_hat_2
}

```


```{r}
set.seed(12)
n <- 100  # Number of rows
d <- 10   # Number of columns
rho <- 0.5  # AR(1) coefficient

# Create the correlation matrix based on AR(1) structure
cor_matrix <- matrix(nrow = d, ncol = d)
for (i in 1:d) {
  for (j in 1:d) {
    cor_matrix[i, j] <- rho^abs(i - j)
  }
}

# Perform Cholesky decomposition of the correlation matrix
cholesky_factor <- chol(cor_matrix)

# Generate independent standard normal variables
random_normals <- matrix(rnorm(n * d), nrow = n, ncol = d)

# Transform the independent normals using the Cholesky factor to get correlated normals
correlated_normals <- random_normals %*% cholesky_factor

# The 'correlated_normals' matrix now has the desired properties
print(correlated_normals)

```

```{r}
set.seed(12)
num_iterations <- 500
n <- 100  # Number of rows
d <- 10   # Number of columns
a <- 2 # signal strength
num_true_signals <- ceiling(.1 * d)
rho <- 0.5  # AR(1) coefficient
for (m in num_iterations){
  cor_matrix <- matrix(nrow = d, ncol = d)
for (i in 1:d) {
  for (j in 1:d) {
    cor_matrix[i, j] <- rho^abs(i - j)
  }
}

# Perform Cholesky decomposition of the correlation matrix
cholesky_factor <- chol(cor_matrix)

# Generate independent standard normal variables
random_normals <- matrix(rnorm(n * d), nrow = n, ncol = d)

# Transform the independent normals using the Cholesky factor to get correlated normals
X <- random_normals %*% cholesky_factor

nonzero_indices <- sample(d, num_true_signals) # randomly sample some indices to be the true signal indices

beta_true <- a * (1:d %in% nonzero_indices) # all the other positions in the vector will be 0

Y <- X %*% beta_true + rnorm(n)



}
```



```{r}
get_regression_model <- function(n, d, a, num_true_signals, random_true_signal_indices = FALSE){
  # random_true_signal_indices selects which indices will be true signals randomly
  # n = sample size, ie number of rows in the matrix
  # d = number of features
  # a = signal strength
  # num_true_signals = number of nonzero beta coefficients
  # Returns an X matrix, response Y, and the positions of the true signals, as according to the simulation setup described in the beginning of Section 3 of Sarkar and Tang
  
   cor_matrix <- matrix(nrow = d, ncol = d)
for (i in 1:d) {
  for (j in 1:d) {
    cor_matrix[i, j] <- rho^abs(i - j)
  }
}

# Perform Cholesky decomposition of the correlation matrix
cholesky_factor <- chol(cor_matrix)

# Generate independent standard normal variables
random_normals <- matrix(rnorm(n * d), nrow = n, ncol = d)

# Transform the independent normals using the Cholesky factor to get correlated normals
X <- random_normals %*% cholesky_factor



if (random_true_signal_indices){
  nonzero_indices <- sample(d, num_true_signals) # randomly sample some indices to be the true signal indices
  beta_true <- a * (1:d %in% nonzero_indices) # all the other positions in the vector will be 0
}
else{
  nonzero_indices <- 1:num_true_signals
  beta_true <- c(rep(a, times = num_true_signals), rep(0, times = d - num_true_signals))
}

Y <- X %*% beta_true + rnorm(n) # linear regression model

return(list('X' = X, 'Y' = Y, 'nonzero_indices' = nonzero_indices))
}
```


```{r}
get_regression_model(n = 100, d = 10, a = 2, num_true_signals = 1)
```


```{r}
sarkar_tang_method1 <- function(X, Y, alpha){
  d <- ncol(X)
  nu <- nrow(X) - 2 * d # n - 2d assuming n > 2d
  T1 <- get_T_1(X, Y)
  T2 <- get_T_2(X, Y)
  P_t1 <- pf(T1^2, 1, nu, lower.tail = FALSE) # to calculate probabilities with the t^2_n distribution, have to use the F(1, n) distribution
  P_t2 <- pf(T2^2, 1, nu, lower.tail = FALSE)
  P_tilde <- numeric(d)
  for (j in 1:d){
    if (P_t1[j] > sqrt(alpha)){
      P_tilde[j] <- 1
    }
    else{
      P_tilde[j] <- P_t2[j]
    }
  }
  P_tilde_BH <- p.adjust(P_tilde, method = "BH")
  rejected_indices <- which(P_tilde_BH < sqrt(alpha))
  return(rejected_indices) # the indices of the features that the test concludes are nonzero
  
}
```


```{r}
set.seed(12)
fdp_vec <- numeric(num_iterations)
pwr_vec <- c(num_iterations)
for (m in 1:num_iterations){
  num_true_signals <- 20
  d <- 100
  feature_indices <- 1:d
  regression <- get_regression_model(n = 1000, d = d, a = 4, num_true_signals = num_true_signals)
  true_nonzero_indices <- regression$nonzero_indices
  rejected_indices <- sarkar_tang_method1(regression$X, regression$Y, alpha = .05)
  num_rejections <- length(rejected_indices)
  num_correct_rejections <- length(intersect(rejected_indices, true_nonzero_indices))
  num_false_rejections <- length(intersect(feature_indices[-true_nonzero_indices], rejected_indices))
  fdp_vec[m] <- num_false_rejections / max(num_rejections, 1)
  pwr_vec[m] <- num_correct_rejections / max(num_rejections, 1)
}
mean(fdp_vec)
mean(pwr_vec)
```

```{r}
regression <- get_regression_model(n = 100, d = 20, a = 4, num_true_signals = 10)
Sigma <- t(regression$X) %*% regression$X
create.solve_equi(Sigma)
create_equicorrelated_D(regression$X)
```


```{r}
create.fixed(regression$X, method = 'equi')$Xk
```

```{r}
create_equicorrelated_knockoffsv2(regression$X)
```


```{r}
create_equicorrelated_knockoffs(regression$X)
```


```{r}
create_equicorrelated_D(regression$X)
```


```{r}
create_equicorrelated_knockoffsv2 <- function(X, randomize=FALSE){
  # Construct the knockoff according to Equation 1.4.
  s <- create.solve_equi(t(X) %*% X)
  X.svd = decompose(X, randomize)
  s_diff = pmax(0, 2*s - (s/X.svd$d)^2) # can be negative due to numerical error
  X_ko = (X.svd$u %*diag% (X.svd$d - s / X.svd$d) +
          X.svd$u_perp %*diag% sqrt(s_diff)) %*% t(X.svd$v)
  return(X_ko)
}
```



```{r}
create_equicorrelated_knockoffs(regression$X)
```


