---
title: "Sarkar and Tang simulation replication"
author: "Ethan Naegele"
date: "2024-04-19"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
X <- matrix(c(1, 2, 3, 1, 2, 3), nrow = 2, byrow = TRUE)
X
X.svd <- svd(X)
X.svd
```


# Internal functions in knockoffs package for computation purposes

```{r}
# Reduced SVD with canonical sign choice.
# 
# Our convention is that the sign of each vector in U is chosen such that the
# coefficient with the largest absolute value is positive.
canonical_svd = function(X) {
  X.svd = tryCatch({
    svd(X)
  }, warning = function(w){}, error = function(e) {
      stop("SVD failed in the creation of fixed-design knockoffs. Try upgrading R to version >= 3.3.0")
  }, finally = {})
  
  for (j in 1:min(dim(X))) {
    i = which.max(abs(X.svd$u[,j]))
    if (X.svd$u[i,j] < 0) {
      X.svd$u[,j] = -X.svd$u[,j]
      X.svd$v[,j] = -X.svd$v[,j]
  }
    }
  return(X.svd)
}

```


```{r}
#' Compute the SVD of X and construct an orthogonal matrix U_perp such that U_perp * U = 0.
#'  
decompose <- function(X, randomize=FALSE) {
  n = nrow(X); p = ncol(X)
  stopifnot(n >= 2*p)
  
  result = canonical_svd(X)
  Q = qr.Q(qr(cbind(result$u, matrix(0,n,p))))
  u_perp = Q[,(p+1):(2*p)]
  if (randomize) {
      Q = qr.Q(qr(rnorm_matrix(p,p)))
      u_perp = u_perp %*% Q
  }
  result$u_perp = u_perp
  result
}
```


```{r}
create_equicorrelated_D <- function(X, randomize=FALSE) {
  # Compute SVD and U_perp.
  X.svd = decompose(X, randomize)
  
  # Set s = min(2 * smallest eigenvalue of X'X, 1), so that all the correlations
  # have the same value 1-s.
  if (any(X.svd$d <= 1e-5 * max(X.svd$d)))
    stop(paste('Data matrix is rank deficient.',
               'Equicorrelated knockoffs will have no power.'))
  lambda_min = min(X.svd$d)^2
  s = min(2*lambda_min, 1)
  return(s)
}

create_equicorrelated_knockoffs <- function(X, randomize=FALSE){
  # Construct the knockoff according to Equation 1.4.
  s <- create_equicorrelated_D(X)
  s_diff = pmax(0, 2*s - (s/X.svd$d)^2) # can be negative due to numerical error
  X_ko = (X.svd$u %*diag% (X.svd$d - s / X.svd$d) +
          X.svd$u_perp %*diag% sqrt(s_diff)) %*% t(X.svd$v)
  return(X_ko)
}
```

# Functions specific to quantities from Sarkar and Tang

```{r}
# faster function for calculating norm of vector
norm_vec <- function(x) sqrt(sum(x^2))

# obtain the unbiased estimator of the variance parameter tau in the regression setting Y = N(X * beta, tau^2 I_n)
get_variance_parameter_estimate <- function(X, Y){
  n <- nrow(X)
  d <- ncol(X)
  tau_hat_squared <- (norm_vec(Y - X %*% solve(t(X) %*% X) %*% t(X) %*% Y))^2 / (n - d) # numerator is RSS, and assuming n > d
  return(sqrt(tau_hat_squared))
}
```



```{r}
# get the T_1  estimate provided at the beginning of section 2
library(expm)
get_T_1 <- function(X, Y){
  sigma <- solve(t(X) %*% X)
  D <- create_equicorrelated_D(X)
  X_ko <- create_equicorrelated_knockoffs(X)
  beta_hat_1 <- solve(2 * sigma - D) %*% t(X + t(X)) %*% Y
  # tau hat estimated with the columnwise concatenation of X and knockoff X
  T_1 <- (1 / get_variance_parameter_estimate(cbind(X, X_ko), Y) * sqrt(2)) *
    solve(sqrtm(solve(diag(diag(2 * sigma - D))))) %*% beta_hat_1 # unsure about interpretation of diag operator in the equation for T1 from the paper; this is my best interpretation of it - Ignat agrees
}

# get the T_2 estimate 
get_T_2 <- function(X, Y){
  beta_hat_2 <- solve(D) %*% t(X - t(X_ko)) %*% Y
  D <- create_equicorrelated_D(X)
  X_ko <- create_equicorrelated_knockoffs(X)
  T_2 <- (1 / get_variance_parameter_estimate(cbind(X, X_ko), Y) * sqrt(2)) * sqrtm(D) %*% beta_hat_2
}

```


```{r}
set.seed(12)
n <- 100  # Number of rows
d <- 10   # Number of columns
rho <- 0.5  # AR(1) coefficient

# Create the correlation matrix based on AR(1) structure
cor_matrix <- matrix(nrow = d, ncol = d)
for (i in 1:d) {
  for (j in 1:d) {
    cor_matrix[i, j] <- rho^abs(i - j)
  }
}

# Perform Cholesky decomposition of the correlation matrix
cholesky_factor <- chol(cor_matrix)

# Generate independent standard normal variables
random_normals <- matrix(rnorm(n * d), nrow = n, ncol = d)

# Transform the independent normals using the Cholesky factor to get correlated normals
correlated_normals <- random_normals %*% cholesky_factor

# The 'correlated_normals' matrix now has the desired properties
print(correlated_normals)

```

```{r}
set.seed(12)
num_iterations <- 500
n <- 100  # Number of rows
d <- 10   # Number of columns
a <- 2 # signal strength
num_true_signals <- ceiling(.1 * d)
rho <- 0.5  # AR(1) coefficient
for (m in num_iterations){
  cor_matrix <- matrix(nrow = d, ncol = d)
for (i in 1:d) {
  for (j in 1:d) {
    cor_matrix[i, j] <- rho^abs(i - j)
  }
}

# Perform Cholesky decomposition of the correlation matrix
cholesky_factor <- chol(cor_matrix)

# Generate independent standard normal variables
random_normals <- matrix(rnorm(n * d), nrow = n, ncol = d)

# Transform the independent normals using the Cholesky factor to get correlated normals
X <- random_normals %*% cholesky_factor

nonzero_indices <- sample(d, num_true_signals) # randomly sample some indices to be the true signal indices

beta_true <- a * (1:d %in% nonzero_indices) # all the other positions in the vector will be 0

Y <- X %*% beta_true + rnorm(n)



}
```



```{r}
get_regression_model <- function(n, d, a, num_true_signals, random_true_signal_indices = FALSE){
  # random_true_signal_indices selects which indices will be true signals randomly
  # n = sample size, ie number of rows in the matrix
  # d = number of features
  # a = signal strength
  # num_true_signals = number of nonzero beta coefficients
  # Returns an X matrix, response Y, and the positions of the true signals, as according to the simulation setup described in the beginning of Section 3 of Sarkar and Tang
  
   cor_matrix <- matrix(nrow = d, ncol = d)
for (i in 1:d) {
  for (j in 1:d) {
    cor_matrix[i, j] <- rho^abs(i - j)
  }
}

# Perform Cholesky decomposition of the correlation matrix
cholesky_factor <- chol(cor_matrix)

# Generate independent standard normal variables
random_normals <- matrix(rnorm(n * d), nrow = n, ncol = d)

# Transform the independent normals using the Cholesky factor to get correlated normals
X <- random_normals %*% cholesky_factor

nonzero_indices <- sample(d, num_true_signals) # randomly sample some indices to be the true signal indices

if (random_true_signal_indices)
  beta_true <- a * (1:d %in% nonzero_indices) # all the other positions in the vector will be 0
else{
  beta_true <- c(rep(a, times = num_true_signals), rep(0, times = d - num_true_signals))
}

Y <- X %*% beta_true + rnorm(n) # linear regression model

return(list(X, Y, nonzero_indices))
}
```







